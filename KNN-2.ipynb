{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a23d82d-9c8a-4f7b-8f5b-7c4eb215615a",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2be295-a576-46c6-abec-5b36f6df5d82",
   "metadata": {},
   "source": [
    "Ans - Euclidean distance and Manhattan distance are two ways to measure the dissimilarity between data points in KNN. Euclidean distance calculates the straight-line distance, like the shortest path between two points on a map. Manhattan distance, however, calculates the distance by summing the absolute differences in each dimension, like the distance a taxi would travel on city blocks.\n",
    "\n",
    "This difference in calculation leads to different behaviors. Euclidean distance is more sensitive to outliers and differences in feature scales, while Manhattan distance is less affected by these factors. Euclidean distance may also struggle in high-dimensional spaces due to the \"curse of dimensionality,\" where distances become more uniform and less informative.\n",
    "\n",
    "The choice between these two distances depends on the specific dataset and problem. If features are continuous and on similar scales, with few outliers, Euclidean distance might be preferred. If features are discrete, have varying scales, or if outliers are a concern, then Manhattan distance might be more suitable. In high-dimensional spaces, Manhattan distance can sometimes be more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6815d6bc-f8ae-4e31-8240-9642c0c78a64",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2683c9c4-d1a2-4486-a761-985268e0fd40",
   "metadata": {},
   "source": [
    "Ans - 1] Square Root of N: A simple rule of thumb is to start with 'K' as the square root of the number of samples in your dataset. It often provides a decent starting point.\n",
    "\n",
    "2] Odd vs. Even: In classification tasks, using an odd value for 'K' helps avoid ties in voting between classes.\n",
    "\n",
    "3] Cross-Validation: Split your dataset into multiple folds (e.g., 5-fold or 10-fold cross-validation). For each fold, train the KNN model with different 'K' values and evaluate its performance on the remaining fold. Choose the 'K' value that gives the best average performance across all folds.\n",
    "\n",
    "4] Grid Search: Define a range of 'K' values to try (e.g., 1 to 20). Combine this with a grid search over other hyperparameters (like distance metric) to find the best combination that minimizes the error rate on a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe2618a-27b1-437d-b434-c3f826acd202",
   "metadata": {},
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85694fa9-b583-49a7-8241-024b991a6ee9",
   "metadata": {},
   "source": [
    "Ans - The choice between Euclidean and Manhattan distance depends on:\n",
    "\n",
    "1] Data with Varying Scales: If features have vastly different scales, Manhattan distance is preferred due to its lower sensitivity to scale \n",
    "differences, preventing bias towards larger-valued features.\n",
    "\n",
    "2] Categorical or Ordinal Features: For categorical or ordinal data, Manhattan distance is more suitable as it considers the number of steps between categories or levels, treating each feature equally.\n",
    "\n",
    "3] Noise or Outliers: When dealing with noisy or outlier-prone data, Manhattan distance is more robust, as it only considers differences between feature values, not their magnitudes.\n",
    "\n",
    "4] Data Distribution: If data points tend to align along axes, indicating a linear relationship between features, Manhattan distance is often preferred. However, for data distributed in clusters or without a clear linear pattern, Euclidean distance might be better suited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ad605-00f1-46b7-bab2-326f119b4aee",
   "metadata": {},
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213f4f3-2182-4f74-983e-cf03f77f7c71",
   "metadata": {},
   "source": [
    "Ans - 1] Number of Neighbors (k): This is the most critical hyperparameter in KNN. A smaller k makes the model more complex and prone to overfitting, capturing noise and outliers in the training data. A larger k makes the model simpler and less flexible, potentially leading to underfitting by missing important patterns. Finding the optimal k involves balancing this trade-off between bias and variance.\n",
    "\n",
    "2] Distance Metric: The distance metric determines how similarity between data points is measured. Common choices include Euclidean, Manhattan, and Minkowski distance. The best choice depends on the nature of your data and the problem you're solving. Euclidean distance works well for continuous features, while Manhattan distance might be better for discrete or categorical features.\n",
    "\n",
    "3] Weights: This hyperparameter controls whether all neighbors have equal influence on the prediction or if closer neighbors have more weight.  \"Uniform\" weighting treats all neighbors equally, while \"distance\" weighting assigns more importance to closer neighbors. The choice depends on whether you believe closer points are more relevant to the prediction.\n",
    "\n",
    "Hyperparameters to improve - \n",
    "\n",
    "1] Grid Search: Define a grid of possible hyperparameter values and systematically evaluate the model's performance for each combination. This method is exhaustive but can be computationally expensive.\n",
    "\n",
    "2] Random Search: Randomly sample combinations of hyperparameter values from a defined search space. This method is more efficient than grid search for large search spaces.\n",
    "\n",
    "3] Cross-Validation: Use techniques like k-fold cross-validation to get a more robust estimate of model performance. This helps prevent overfitting and gives a better idea of how well the model will generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24d825-acb9-424a-8f12-30bf0dfc05a6",
   "metadata": {},
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e097525-2ea5-4c58-9538-926a065de586",
   "metadata": {},
   "source": [
    "Ans - 1] Small Training Set:\n",
    "\n",
    "a. Overfitting: With too few examples, KNN is prone to overfitting, where it memorizes the training data rather than learning general patterns. This leads to high accuracy on the training set but poor performance on new, unseen data.   \n",
    "\n",
    "b. Limited Representation: A small training set might not adequately represent the underlying data distribution, causing the model to miss crucial variations and relationships.\n",
    "\n",
    "2] Large Training Set:\n",
    "\n",
    "a. Computational Cost: A large training set increases the computational burden during prediction, as KNN needs to calculate distances to all instances.   \n",
    "\n",
    "b. Storage Requirements: Storing a vast training set can also pose challenges in terms of memory and storage resources.\n",
    "\n",
    "c. Diminishing Returns: Beyond a certain point, adding more training data may not significantly improve performance, especially if the additional data is redundant or doesn't introduce new patterns.\n",
    "\n",
    "Techniques to Optimize Training Set Size -\n",
    "\n",
    "a. Cross-Validation: Use techniques like k-fold cross-validation to estimate the optimal training set size. By varying the split ratio between training and validation sets, you can observe how performance changes with different training set sizes.\n",
    "\n",
    "b. Learning Curves: Plot the training and validation error against the training set size. This can help visualize how performance improves with more data and identify the point where adding more data doesn't provide substantial gains.\n",
    "\n",
    "c. Data Augmentation:  If your dataset is too small, consider augmenting it with additional synthetic data generated through techniques like rotation, scaling, or adding noise. However, be careful not to over-augment, as it can introduce bias.\n",
    "\n",
    "d. Ensemble Methods: Combine multiple KNN models trained on different subsets of the data to improve overall performance. This can be done using techniques like bagging or boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37456144-0fd7-44dc-97e5-8685505118cf",
   "metadata": {},
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8039b6fc-135b-463d-baac-38827156f1aa",
   "metadata": {},
   "source": [
    "Ans - 1] Computational Cost: KNN can be computationally expensive, especially with large datasets, as it needs to calculate distances to all training instances for each prediction.\n",
    "\n",
    "Solution:\n",
    "\n",
    "a. Use approximate nearest neighbor algorithms like Annoy or HNSW to speed up distance calculations.\n",
    "\n",
    "b. Employ dimensionality reduction techniques like PCA or feature selection to reduce the number of features.\n",
    "\n",
    "c. Consider using KNN for smaller datasets or when real-time prediction speed is not critical.\n",
    "\n",
    "2] Curse of Dimensionality: KNN's performance can degrade in high-dimensional spaces due to the increased sparsity of data points.\n",
    "\n",
    "Solution:\n",
    "\n",
    "a. Use dimensionality reduction techniques like PCA or feature selection to reduce the number of features.\n",
    "\n",
    "b. Choose a distance metric that is less affected by dimensionality, such as Manhattan distance or cosine similarity.\n",
    "\n",
    "3] Sensitivity to Irrelevant Features: The presence of irrelevant or noisy features can negatively impact KNN's performance.\n",
    "\n",
    "Solution:\n",
    "\n",
    "a. Use feature selection techniques to identify and remove irrelevant features.\n",
    "\n",
    "b. Apply feature engineering to create new, more informative features.\n",
    "\n",
    "4] Sensitivity to Outliers: Outliers can significantly influence KNN's predictions.\n",
    "\n",
    "Solution:\n",
    "\n",
    "a. Identify and remove outliers before training the model.\n",
    "\n",
    "b. Use a robust distance metric, such as Manhattan distance, which is less sensitive to outliers.\n",
    "\n",
    "5] Imbalanced Classes: KNN may struggle with imbalanced datasets where one class has significantly more instances than others.\n",
    "\n",
    "Solution:\n",
    "\n",
    "a. Use oversampling or undersampling techniques to balance the class distribution.\n",
    "\n",
    "b. Assign different weights to different classes during prediction to account for the imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f874a2-070a-40a9-8643-417b994c21a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c524114-b572-4df9-adda-fa3730d71fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
